import sys
import time
import csv
import json
import argparse
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from multiprocessing import Pool, set_start_method, Manager
from functools import partial

from src.utils import *
from src.paths import *
from src.config import *
from src.worker import ImageWorker
from logs.logger import CSVLogger
from model.SR_Script.super_resolution import SA_SuperResolution
from benchmark.benchmark import benchmark

MAX_ATTEMPTS = 10
RETRY_DELAY = 5  # seconds


def chunk_images(images, n_chunks):
    avg = len(images) / float(n_chunks)
    chunks = []
    last = 0.0
    while last < len(images):
        chunk = images[int(last):int(last + avg)]
        if chunk:
            chunks.append(chunk)
        last += avg
    return chunks


def process_batch(images, threads, super_resolution_dir, downscaling_dir, model_path, logger_path, progress_queue):
    print(f"Process_batch: elaboro {len(images)} immagini")
    model = SA_SuperResolution(
        models_dir=model_path,
        model_scale=SUPER_RESOLUTION_PAR,
        tile_size=128,
        gpu_id=0,
        verbosity=False,
    )

    logger = CSVLogger(logger_path)
    worker = ImageWorker(logger, super_resolution_dir, downscaling_dir, model)

    with ThreadPoolExecutor(max_workers=threads) as executor:
        futures = {executor.submit(worker.run, img): img for img in images}

        for future in as_completed(futures):
            img = futures[future]
            try:
                future.result()
            except Exception as e:
                logger.log(img, "run", success=False, error=f"Thread error: {e}")
            finally:
                progress_queue.put(1)  # segnala un'immagine completata

    logger.stop()


def run_standard_processing(processes, threads):
    print("ðŸ” Caricamento modello di super-risoluzione (test iniziale)...")
    try:
        _ = SA_SuperResolution(
            models_dir=SR_SCRIPT_MODEL_DIR,
            model_scale=SUPER_RESOLUTION_PAR,
            tile_size=128,
            gpu_id=0,
            verbosity=True,
        )
    except Exception as e:
        raise RuntimeError(f"Errore nel caricamento modello SR: {e}")

    print("\nðŸ“‚ Scansione cartelle da elaborare...")

    super_resolution_dir, downscaling_dir = find_output_dir()
    folders = [f for f in INPUT_IMAGES_DIR.rglob("*") if f.is_dir()]
    folder_to_images = {}

    for folder in folders:
        images = [
            img for img in folder.glob("*")
            if is_valid_image_file(img)
        ]
        images_to_process = []
        for img in images:
            rel = img.relative_to(INPUT_IMAGES_DIR)
            subdir = rel.parent
            out_path = downscaling_dir / subdir / img.name
            if not out_path.exists():
                images_to_process.append(img)
        if images_to_process:
            folder_to_images[folder] = images_to_process

    if not folder_to_images:
        print("âœ… Tutte le immagini risultano giÃ  elaborate.")
        return

    total_success = 0
    total_error = 0

    for folder, images in folder_to_images.items():
        print(f"\nâž¡ï¸  Cartella: {folder} ({len(images)} immagini da processare)")

        chunks = chunk_images(images, processes)
        print(f"DEBUG: {len(images)} immagini suddivise in {len(chunks)} chunk")
        for i, chunk in enumerate(chunks):
            print(f"DEBUG: chunk {i} contiene {len(chunk)} immagini")

        manager = Manager()
        progress_queue = manager.Queue()

        target = partial(
            process_batch,
            threads=threads,
            super_resolution_dir=super_resolution_dir,
            downscaling_dir=downscaling_dir,
            model_path=SR_SCRIPT_MODEL_DIR,
            logger_path=CSV_LOG_PATH,
            progress_queue=progress_queue,
        )

        try:
            set_start_method("spawn", force=True)
            with Pool(processes) as pool:
                result = pool.map_async(target, chunks)

                with tqdm(total=len(images), desc="ðŸ“· Immagini elaborate", ncols=80) as pbar:
                    completed = 0
                    while completed < len(images):
                        try:
                            progress_queue.get(timeout=0.5)
                            completed += 1
                            pbar.update(1)
                        except:
                            if result.ready():
                                break
                    result.wait()
        except KeyboardInterrupt:
            print("\n[ðŸšª] Interrotto manualmente dall'utente. Uscita.")
            sys.exit(0)

        folder_error_count = 0
        if CSV_LOG_PATH.exists():
            with open(CSV_LOG_PATH, "r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                folder_error_count = sum(
                    1 for row in reader
                    if row["status"] == "false" and Path(row["filename"]).parent.name == folder.name
                )

        folder_success = len(images) - folder_error_count
        total_success += folder_success
        total_error += folder_error_count

        print(f"   âœ… Successi: {folder_success} | âŒ Errori: {folder_error_count}")

    print("\nðŸ“Š Risultato finale:")
    print(f"âœ… Immagini processate con successo: {total_success}")
    print(f"âŒ Immagini con errore:              {total_error}")


def main():
    parser = argparse.ArgumentParser(description="Processa immagini o esegui benchmark.")
    parser.add_argument("--benchmark", action="store_true", help="Esegui benchmark multiprocesso e multithread")
    args = parser.parse_args()

    if args.benchmark:
        benchmark()
        return

    if not JSON_BENCHMARK_BEST_CONFIG_PATH.exists():
        print("âš ï¸ Nessuna configurazione ottimale trovata. Eseguo benchmark...")
        benchmark()

    with JSON_BENCHMARK_BEST_CONFIG_PATH.open("r", encoding="utf-8") as f:
        best_config = json.load(f)

    processes = int(best_config["processes"])
    threads = int(best_config["threads"])
    print(f"\nðŸ“Œ Uso della configurazione ottimale: {processes} processi, {threads} thread")

    try:
        for attempt in range(1, MAX_ATTEMPTS + 1):
            if CSV_LOG_PATH.exists():
                CSV_LOG_PATH.unlink()

            try:
                print(f"\nðŸ” Tentativo {attempt} di {MAX_ATTEMPTS}...\n")
                run_standard_processing(processes, threads)
                print("âœ… Elaborazione completata con successo.")
                break
            except KeyboardInterrupt:
                print("\n[ðŸšª] Interrotto manualmente dall'utente. Uscita.")
                sys.exit(0)
            except Exception as e:
                print(f"\nâŒ Crash: {e}")
                if attempt < MAX_ATTEMPTS:
                    print(f"â³ Nuovo tentativo in {RETRY_DELAY} secondi...")
                    time.sleep(RETRY_DELAY)
                else:
                    print("\nâŒ Numero massimo di tentativi raggiunto. Uscita.")
                    sys.exit(1)
    except KeyboardInterrupt:
        print("\n[ðŸšª] Interrotto manualmente dall'utente. Uscita.")
        sys.exit(0)


if __name__ == "__main__":
    main()
